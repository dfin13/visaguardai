#!/usr/bin/env python
"""
Twitter/X Analysis Pipeline Diagnostic

Purpose: Understand the current state of the Twitter scraper without making any changes.

Tests:
1. Actor ID and configuration
2. Field extraction (text, URL, timestamp, engagement)
3. Lightweight test run with @elonmusk (2 posts)
4. JSON fields returned
5. Integration with analyzer
6. Template compatibility

DO NOT MODIFY ANY FILES OR PUSH CHANGES - DIAGNOSTIC ONLY
"""

import os
import sys
import json
import time
import django

# Django setup
sys.path.append('/Users/davidfinney/Downloads/visaguardai')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'visaguardai.settings')
django.setup()

from apify_client import ApifyClient
from apify_client._errors import ApifyApiError
from dashboard.models import Config
from dotenv import load_dotenv

# Load environment
load_dotenv()

print("\n" + "="*80)
print("ğŸ” TWITTER/X PIPELINE DIAGNOSTIC")
print("="*80 + "\n")

# ============================================================================
# STEP 1: Check API Key Configuration
# ============================================================================

print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("STEP 1: API KEY VERIFICATION")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

env_key = os.getenv('APIFY_API_KEY')
db_key = None

try:
    config = Config.objects.first()
    if config:
        db_key = config.Apify_api_key
except Exception as e:
    print(f"âš ï¸  Could not retrieve key from Config table: {e}")

print(f"ğŸ“ .env file key: {env_key[:20] if env_key else 'NOT FOUND'}...{env_key[-5:] if env_key else ''}")
print(f"ğŸ—„ï¸  Database Config key: {db_key[:20] if db_key else 'NOT FOUND'}...{db_key[-5:] if db_key else ''}")

# Determine which key t.py will use (from the code inspection)
apify_key = env_key or db_key
if not apify_key:
    print("\nâŒ FATAL: No Apify API key found in either .env or database")
    sys.exit(1)

print(f"\nâœ… Twitter scraper will use: {'âœ… .env key' if env_key else 'ğŸ—„ï¸  Database key'}")
print(f"   Key ends with: ...{apify_key[-8:]}")

# Initialize Apify client
client = ApifyClient(apify_key)

# Check Apify account permissions
print("\nğŸ“Š Checking Apify account permissions...")
try:
    user_info = client.user().get()
    print(f"   Account ID: {user_info.get('id', 'Unknown')}")
    print(f"   Username: {user_info.get('username', 'Unknown')}")
    
    # Check features
    features = user_info.get('plan', {}).get('availableFeatures', [])
    print(f"\nğŸ¯ Available Features:")
    for feature in features:
        icon = "âœ…" if feature in ["ACTORS_PUBLIC_ALL", "PAID_ACTORS"] else "  "
        print(f"   {icon} {feature}")
    
    has_public_access = "ACTORS_PUBLIC_ALL" in features
    has_paid_access = "PAID_ACTORS" in features
    
    if has_public_access:
        print(f"\nâœ… Can run public actors (including danek/twitter-timeline)")
    else:
        print(f"\nâš ï¸  Cannot run public actors - may need paid actor or different approach")
        
except ApifyApiError as e:
    print(f"âŒ Apify API Error: {e}")
    print(f"   Type: {type(e).__name__}")
    sys.exit(1)

# ============================================================================
# STEP 2: Inspect Twitter Scraper Code Configuration
# ============================================================================

print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("STEP 2: TWITTER SCRAPER CODE INSPECTION")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

print("ğŸ“„ File: dashboard/scraper/t.py")
print("   Function: analyze_twitter_profile()")
print()
print("ğŸ”§ Current Configuration:")
print("   â€¢ Actor ID: danek/twitter-timeline")
print("   â€¢ Input Fields:")
print("      - usernames: [username] (list)")
print("      - max_posts: tweets_desired (int)")
print("      - include_replies: False")
print("      - include_user_info: True")
print("      - max_request_retries: 3")
print("      - request_timeout_secs: 30")
print()
print("ğŸ“¦ Data Extraction Logic:")
print("   â€¢ Tweet text: full_text OR text OR content")
print("   â€¢ Timestamp: NOT extracted from dataset")
print("   â€¢ Likes: NOT extracted")
print("   â€¢ Replies: NOT extracted")
print("   â€¢ Post URL: âŒ NOT EXTRACTED")
print()
print("ğŸ”„ Passed to Analyzer:")
print("   â€¢ caption: tweet text")
print("   â€¢ text: tweet text")
print("   â€¢ post_text: tweet text")
print("   â€¢ created_at: tweet.get('timestamp') - but NOT extracted above!")
print("   â€¢ likes_count: tweet.get('likes', 0) - but NOT extracted!")
print("   â€¢ comments_count: tweet.get('replies', 0) - but NOT extracted!")
print("   â€¢ type: 'tweet'")
print("   â€¢ hashtags: tweet.get('hashtags', []) - but NOT extracted!")
print("   â€¢ mentions: tweet.get('mentions', []) - but NOT extracted!")
print()
print("âš ï¸  FINDING: Metadata fields (URL, timestamp, engagement) are referenced")
print("             but NOT actually extracted from the Apify dataset!")

# ============================================================================
# STEP 3: Run Lightweight Test with @elonmusk
# ============================================================================

print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("STEP 3: LIVE SCRAPING TEST")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

test_username = "elonmusk"
test_limit = 2

print(f"ğŸ¯ Test Target: @{test_username}")
print(f"ğŸ“Š Post Limit: {test_limit}")
print()

actor_id = "danek/twitter-timeline"
run_input = {
    "usernames": [test_username],
    "max_posts": test_limit,
    "include_replies": False,
    "include_user_info": True,
    "max_request_retries": 3,
    "request_timeout_secs": 30,
}

print(f"ğŸš€ Calling actor: {actor_id}")
print(f"   Input: {json.dumps(run_input, indent=6)}")
print()

try:
    print("â³ Starting actor run... (this may take 10-20 seconds)")
    start_time = time.time()
    
    run = client.actor(actor_id).call(run_input=run_input, wait_secs=60)
    
    elapsed = time.time() - start_time
    print(f"âœ… Actor run completed in {elapsed:.1f} seconds")
    print(f"   Run ID: {run.get('id')}")
    print(f"   Status: {run.get('status')}")
    print(f"   Default Dataset ID: {run.get('defaultDatasetId')}")
    print()
    
    # Wait a bit for dataset to populate
    print("â³ Waiting 3 seconds for dataset to populate...")
    time.sleep(3)
    
    # Fetch dataset items
    print("ğŸ“¥ Fetching dataset items...")
    dataset_items = client.dataset(run["defaultDatasetId"]).list_items().items
    
    print(f"âœ… Retrieved {len(dataset_items)} items from dataset")
    print()
    
    if len(dataset_items) == 0:
        print("âš ï¸  WARNING: No items returned from actor")
        print("   Possible reasons:")
        print("   â€¢ Account is private/suspended")
        print("   â€¢ Rate limiting")
        print("   â€¢ Actor configuration issue")
        print()
    else:
        # ============================================================================
        # STEP 4: Analyze Raw Data Structure
        # ============================================================================
        
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("STEP 4: RAW DATA STRUCTURE ANALYSIS")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        print("ğŸ” Examining first tweet item structure...")
        print()
        
        first_item = dataset_items[0]
        print(f"ğŸ“¦ Available fields in first item ({len(first_item)} total):")
        print()
        
        # Categorize fields
        text_fields = []
        url_fields = []
        timestamp_fields = []
        engagement_fields = []
        user_fields = []
        other_fields = []
        
        for key in first_item.keys():
            key_lower = key.lower()
            if any(word in key_lower for word in ['text', 'content', 'caption']):
                text_fields.append(key)
            elif any(word in key_lower for word in ['url', 'link', 'permalink']):
                url_fields.append(key)
            elif any(word in key_lower for word in ['time', 'date', 'created']):
                timestamp_fields.append(key)
            elif any(word in key_lower for word in ['like', 'reply', 'retweet', 'quote', 'view', 'engagement']):
                engagement_fields.append(key)
            elif any(word in key_lower for word in ['user', 'author', 'name', 'screen']):
                user_fields.append(key)
            else:
                other_fields.append(key)
        
        print("ğŸ“ TEXT FIELDS:")
        for field in text_fields:
            value = first_item.get(field)
            if isinstance(value, str):
                preview = value[:100] + "..." if len(value) > 100 else value
                print(f"   â€¢ {field}: \"{preview}\"")
            else:
                print(f"   â€¢ {field}: {type(value).__name__}")
        print()
        
        print("ğŸ”— URL FIELDS:")
        if url_fields:
            for field in url_fields:
                value = first_item.get(field)
                print(f"   â€¢ {field}: {value}")
        else:
            print("   âŒ NO URL FIELDS FOUND")
        print()
        
        print("â° TIMESTAMP FIELDS:")
        if timestamp_fields:
            for field in timestamp_fields:
                value = first_item.get(field)
                print(f"   â€¢ {field}: {value}")
        else:
            print("   âŒ NO TIMESTAMP FIELDS FOUND")
        print()
        
        print("ğŸ“Š ENGAGEMENT FIELDS:")
        if engagement_fields:
            for field in engagement_fields:
                value = first_item.get(field)
                print(f"   â€¢ {field}: {value}")
        else:
            print("   âŒ NO ENGAGEMENT FIELDS FOUND")
        print()
        
        print("ğŸ‘¤ USER FIELDS:")
        for field in user_fields:
            value = first_item.get(field)
            if isinstance(value, dict):
                print(f"   â€¢ {field}: (dict with {len(value)} keys)")
            elif isinstance(value, str) and len(value) < 50:
                print(f"   â€¢ {field}: {value}")
            else:
                print(f"   â€¢ {field}: {type(value).__name__}")
        print()
        
        print("ğŸ”§ OTHER FIELDS:")
        for field in other_fields[:10]:  # Limit to first 10
            value = first_item.get(field)
            if isinstance(value, (str, int, float, bool)) and not isinstance(value, dict):
                print(f"   â€¢ {field}: {value}")
            else:
                print(f"   â€¢ {field}: {type(value).__name__}")
        if len(other_fields) > 10:
            print(f"   ... and {len(other_fields) - 10} more fields")
        print()
        
        # Show full first item structure
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("FULL FIRST ITEM (JSON):")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        print(json.dumps(first_item, indent=2, ensure_ascii=False)[:2000])
        if len(json.dumps(first_item, indent=2, ensure_ascii=False)) > 2000:
            print("\n... (truncated, showing first 2000 chars)")
        print()
        
        # ============================================================================
        # STEP 5: Test Current Extraction Logic
        # ============================================================================
        
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("STEP 5: CURRENT EXTRACTION LOGIC TEST")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        print("ğŸ§ª Simulating current t.py extraction logic...")
        print()
        
        tweets = []
        for item in dataset_items:
            # Current extraction logic from t.py
            tweet_text = item.get("full_text") or item.get("text") or item.get("content")
            if tweet_text:
                tweets.append({"tweet": tweet_text})
        
        print(f"âœ… Extracted {len(tweets)} tweets using current logic")
        print()
        
        for i, tweet in enumerate(tweets, 1):
            print(f"Tweet #{i}:")
            preview = tweet['tweet'][:100] + "..." if len(tweet['tweet']) > 100 else tweet['tweet']
            print(f"   Text: \"{preview}\"")
            print()
        
        # Check what data is passed to analyzer
        print("ğŸ”„ Simulating data preparation for analyzer...")
        print()
        
        tweets_data = []
        for tweet in tweets:
            tweet_data = {
                'caption': tweet.get('tweet', ''),
                'text': tweet.get('tweet', ''),
                'post_text': tweet.get('tweet', ''),
                'created_at': tweet.get('timestamp'),  # Will be None!
                'likes_count': tweet.get('likes', 0),  # Will be 0!
                'comments_count': tweet.get('replies', 0),  # Will be 0!
                'type': 'tweet',
                'hashtags': tweet.get('hashtags', []),  # Will be []!
                'mentions': tweet.get('mentions', []),  # Will be []!
            }
            tweets_data.append(tweet_data)
        
        print("ğŸ“¦ Sample tweet_data object passed to analyzer:")
        print(json.dumps(tweets_data[0], indent=2))
        print()
        
        print("âš ï¸  FINDINGS:")
        print("   â€¢ created_at: None (timestamp not extracted)")
        print("   â€¢ likes_count: 0 (likes not extracted)")
        print("   â€¢ comments_count: 0 (replies not extracted)")
        print("   â€¢ hashtags: [] (hashtags not extracted)")
        print("   â€¢ mentions: [] (mentions not extracted)")
        print("   â€¢ post_url: âŒ NOT INCLUDED AT ALL")
        print()
        
        # ============================================================================
        # STEP 6: Template Compatibility Check
        # ============================================================================
        
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("STEP 6: TEMPLATE COMPATIBILITY CHECK")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        print("ğŸ“„ Template: templates/dashboard/result.html")
        print()
        print("âœ… Twitter section found in template:")
        print("   â€¢ Platform card with Twitter icon")
        print("   â€¢ Risk badge (Low/Moderate/High)")
        print("   â€¢ Tweet content display")
        print("   â€¢ Three analysis sections (Reinforcement, Suppression, Flag)")
        print()
        print("ğŸ”— Post URL display:")
        print("   â€¢ Template checks: {% if item.post_data.post_url %}")
        print("   â€¢ Displays: 'View original post' link")
        print("   â€¢ Current status: âŒ WILL NOT DISPLAY (post_url not extracted)")
        print()
        
        # ============================================================================
        # STEP 7: Summary & Recommendations
        # ============================================================================
        
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("STEP 7: DIAGNOSTIC SUMMARY")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        print("âœ… WHAT WORKS:")
        print("   â€¢ Actor runs successfully")
        print("   â€¢ Basic tweet text extraction")
        print("   â€¢ Integration with AI analyzer")
        print("   â€¢ Template display support")
        print("   â€¢ Error handling")
        print()
        
        print("âš ï¸  WHAT'S MISSING:")
        print("   â€¢ Post URLs not extracted")
        print("   â€¢ Timestamps not extracted")
        print("   â€¢ Engagement metrics (likes, replies, retweets) not extracted")
        print("   â€¢ Hashtags not extracted")
        print("   â€¢ Mentions not extracted")
        print()
        
        print("ğŸ” COMPARISON TO OTHER PLATFORMS:")
        print()
        print("   Instagram: âœ… Full metadata (URL, timestamp, engagement)")
        print("   LinkedIn:  âœ… Full metadata (URL, timestamp, engagement)")
        print("   Facebook:  âœ… Full metadata (URL, timestamp, engagement)")
        print("   Twitter:   âš ï¸  TEXT ONLY - missing metadata")
        print()
        
        print("ğŸ“‹ RECOMMENDED ACTIONS TO ACHIEVE PARITY:")
        print()
        print("   1. Update t.py data extraction:")
        print("      â€¢ Extract URL fields from Apify response")
        print("      â€¢ Extract timestamp fields")
        print("      â€¢ Extract engagement fields (likes, replies, retweets)")
        print("      â€¢ Extract hashtags and mentions")
        print()
        print("   2. Update tweets_data preparation:")
        print("      â€¢ Add 'post_url': extracted_url")
        print("      â€¢ Add 'timestamp': extracted_timestamp")
        print("      â€¢ Update engagement counts with real data")
        print()
        print("   3. Test with this diagnostic script")
        print()
        print("   4. Deploy changes following same process as Facebook")
        print()
        
        print("ğŸ¯ READINESS ASSESSMENT:")
        print()
        print("   Core Functionality:     âœ… READY (text extraction & analysis)")
        print("   Metadata Extraction:    âš ï¸  INCOMPLETE")
        print("   Template Display:       âœ… READY (supports all fields)")
        print("   'View Post' Links:      âŒ NOT WORKING (URL missing)")
        print()
        print("   Overall Status: ğŸŸ¡ PARTIALLY READY")
        print("   Estimated work to achieve parity: 30-45 minutes")
        print()

except ApifyApiError as e:
    print(f"\nâŒ APIFY API ERROR:")
    print(f"   Type: {type(e).__name__}")
    print(f"   Message: {e}")
    print()
    
    # Check for specific error types
    error_str = str(e).lower()
    if "plan" in error_str or "subscription" in error_str:
        print("   ğŸ’¡ Issue: Plan/subscription limitation")
        print("      The actor may require a paid plan or different actor")
    elif "not found" in error_str:
        print("   ğŸ’¡ Issue: Actor not found")
        print("      The actor ID may be incorrect or deprecated")
    elif "rate" in error_str or "limit" in error_str:
        print("   ğŸ’¡ Issue: Rate limiting")
        print("      Try again in a few minutes")
    
    print()

except Exception as e:
    import traceback
    print(f"\nâŒ UNEXPECTED ERROR:")
    print(f"   Type: {type(e).__name__}")
    print(f"   Message: {e}")
    print()
    print("Full traceback:")
    print(traceback.format_exc())

print("="*80)
print("âœ… DIAGNOSTIC COMPLETE")
print("="*80)
print()
print("Next steps: Review findings above and plan updates to achieve parity")
print("with Instagram, LinkedIn, and Facebook scrapers.")
print()




