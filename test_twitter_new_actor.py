#!/usr/bin/env python
"""
Test the new apidojo/twitter-scraper-lite actor to verify it works
and understand its data structure before updating t.py
"""

import os
import sys
import json
import time
import django

# Django setup
sys.path.append('/Users/davidfinney/Downloads/visaguardai')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'visaguardai.settings')
django.setup()

from apify_client import ApifyClient
from apify_client._errors import ApifyApiError
from dotenv import load_dotenv

load_dotenv()

print("\n" + "="*80)
print("ğŸ§ª TESTING NEW TWITTER ACTOR: apidojo/twitter-scraper-lite")
print("="*80 + "\n")

# Get API key
apify_key = os.getenv('APIFY_API_KEY')
print(f"âœ… Using API key: ...{apify_key[-8:]}")

# Initialize client
client = ApifyClient(apify_key)

# Test configuration
test_username = "elonmusk"
test_limit = 2

print(f"\nğŸ¯ Test Target: @{test_username}")
print(f"ğŸ“Š Tweet Limit: {test_limit}")
print()

# New actor configuration
actor_id = "apidojo/twitter-scraper-lite"
run_input = {
    "searchTerms": [f"from:{test_username}"],
    "maxTweets": test_limit,
    "addUserInfo": True,
    "includeSearchTerms": False,
}

print(f"ğŸš€ Actor: {actor_id}")
print(f"ğŸ“¦ Input Configuration:")
print(json.dumps(run_input, indent=2))
print()

try:
    print("â³ Starting actor run...")
    start_time = time.time()
    
    run = client.actor(actor_id).call(run_input=run_input, wait_secs=60)
    
    elapsed = time.time() - start_time
    print(f"âœ… Actor run completed in {elapsed:.1f} seconds")
    print(f"   Run ID: {run.get('id')}")
    print(f"   Status: {run.get('status')}")
    print()
    
    # Wait for dataset
    print("â³ Waiting 3 seconds for dataset to populate...")
    time.sleep(3)
    
    # Fetch dataset
    print("ğŸ“¥ Fetching dataset items...")
    dataset_items = client.dataset(run["defaultDatasetId"]).list_items().items
    
    print(f"âœ… Retrieved {len(dataset_items)} items")
    print()
    
    if len(dataset_items) == 0:
        print("âš ï¸  WARNING: No items returned")
    else:
        # Analyze first item structure
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("DATA STRUCTURE ANALYSIS")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        first_item = dataset_items[0]
        
        print(f"ğŸ“¦ Available fields ({len(first_item)} total):")
        for key in sorted(first_item.keys()):
            value = first_item.get(key)
            if isinstance(value, str) and len(value) < 100:
                print(f"   â€¢ {key}: {value}")
            elif isinstance(value, (int, float, bool)):
                print(f"   â€¢ {key}: {value}")
            elif value is None:
                print(f"   â€¢ {key}: None")
            else:
                print(f"   â€¢ {key}: {type(value).__name__}")
        print()
        
        # Check for key fields
        print("ğŸ” KEY FIELD EXTRACTION TEST:")
        print()
        
        # Text
        text = first_item.get("full_text") or first_item.get("text") or first_item.get("content")
        print(f"   âœ… TEXT: {text[:100] if text else 'NOT FOUND'}...")
        print()
        
        # URL
        url = first_item.get("url") or first_item.get("link") or first_item.get("permalink")
        print(f"   {'âœ…' if url else 'âŒ'} URL: {url or 'NOT FOUND'}")
        print()
        
        # Timestamp
        timestamp = first_item.get("created_at") or first_item.get("createdAt") or first_item.get("date")
        print(f"   {'âœ…' if timestamp else 'âŒ'} TIMESTAMP: {timestamp or 'NOT FOUND'}")
        print()
        
        # Engagement
        likes = first_item.get("favorite_count") or first_item.get("likes") or first_item.get("favoriteCount")
        replies = first_item.get("reply_count") or first_item.get("replies") or first_item.get("replyCount")
        retweets = first_item.get("retweet_count") or first_item.get("retweets") or first_item.get("retweetCount")
        
        print(f"   {'âœ…' if likes is not None else 'âŒ'} LIKES: {likes if likes is not None else 'NOT FOUND'}")
        print(f"   {'âœ…' if replies is not None else 'âŒ'} REPLIES: {replies if replies is not None else 'NOT FOUND'}")
        print(f"   {'âœ…' if retweets is not None else 'âŒ'} RETWEETS: {retweets if retweets is not None else 'NOT FOUND'}")
        print()
        
        # Show full first item
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("FULL FIRST ITEM (JSON):")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        print(json.dumps(first_item, indent=2, ensure_ascii=False))
        print()
        
        # Test extraction logic
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("RECOMMENDED EXTRACTION CODE:")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        print("""
tweets = []
for item in dataset_items:
    tweet_text = item.get("full_text") or item.get("text") or item.get("content")
    if tweet_text:
        tweets.append({
            "tweet": tweet_text,
            "post_url": item.get("url") or item.get("link"),
            "timestamp": item.get("created_at") or item.get("createdAt"),
            "likes": item.get("favorite_count", 0) or item.get("favoriteCount", 0),
            "replies": item.get("reply_count", 0) or item.get("replyCount", 0),
            "retweets": item.get("retweet_count", 0) or item.get("retweetCount", 0),
            "hashtags": item.get("hashtags", []),
            "mentions": item.get("user_mentions", []) or item.get("mentions", []),
        })
        """)
        
        print()
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("âœ… TEST SUCCESSFUL!")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        
        print("The apidojo/twitter-scraper-lite actor works correctly.")
        print("You can now update t.py with:")
        print(f'  â€¢ Actor ID: "{actor_id}"')
        print('  â€¢ Input: searchTerms=[f"from:{username}"], maxTweets=tweets_desired')
        print("  â€¢ Data extraction: Use recommended code above")
        print()

except ApifyApiError as e:
    print(f"\nâŒ APIFY API ERROR:")
    print(f"   Type: {type(e).__name__}")
    print(f"   Message: {e}")
    print()
    
    error_str = str(e).lower()
    if "plan" in error_str or "subscription" in error_str or "rent" in error_str:
        print("   ğŸ’¡ This actor also requires a paid plan or rental")
    elif "not found" in error_str:
        print("   ğŸ’¡ Actor not found")
    
except Exception as e:
    import traceback
    print(f"\nâŒ UNEXPECTED ERROR:")
    print(f"   Type: {type(e).__name__}")
    print(f"   Message: {e}")
    print()
    print("Full traceback:")
    print(traceback.format_exc())

print("="*80)
print("âœ… TEST COMPLETE")
print("="*80)




